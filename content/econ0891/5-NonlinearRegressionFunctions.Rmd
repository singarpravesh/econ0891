---
title: "5: Nonlinear Regression Functions"
author: "pravesh"
date: "2021-03-27"
output: html_document
---

# Contents
[5.1 A General Strategy for Modelling Nonlinear Regression Functions](#five-one)<br>
[5.2 Interactions between independent variables](#five-two)<br>
[5.3 Example: The demand for economics journals](#five-three)<br>
[5.4 Nonlinear Effects on Test Scores of the Student-Teacher Ratio](#five-four)<br>
[5.5 Exercise](#five-five)


# 5.1 A General Strategy for Modelling Nonlinear Regression Functions {#five-one}

What is wrong with the linear fit?
```{r, echo=FALSE}
suppressMessages(library("tibble"))
# data
caschool <- readxl::read_xlsx('~/blogdown/econ0891/content/econ0891/1-LinearRegressionWithSingleRegressor/caschool.xlsx')

# fit the model
plot(testscr~ avginc, data = caschool, xlab = "Average Income", ylab = "Test score")
abline(
  lm(testscr ~ avginc, data = caschool)
)
```

>- The linear regression line seems to overestimate the true relationship when income is very high or very low and underestimates it for the middle income group.

## {.smaller}

**The quadratic regression model**

$$
\text{testscr}=\beta_0 + \beta_1\text{avginc}+ \beta_2\text{avginc}^2 + u
$$

```{r}
# the quadratic model
quad_model <- lm(testscr ~ avginc + I(avginc^2), data = caschool)

# heteroscedasticity-robust model summary
lmtest::coeftest(quad_model, vcov. = sandwich::vcovHC(quad_model, 
                                                      type = "HC1"))
```

---

```{r}
# assign id's to the values of avginc in increasing order
order_id <- order(caschool$avginc) 
# draw lines with values ordered in ascending order and plot
plot(testscr~ avginc, data = caschool, 
     xlab = "Average Income", 
     ylab = "Test score")
abline(lm(testscr ~ avginc, data = caschool))
lines(x = caschool$avginc[order_id], 
      y = fitted(quad_model)[order_id], col = "red")
legend("bottomright",legend = c("linear model", "quadratic model"),
       col = c("black", "red"),
       lty = 1)
```


## {.smaller}

**Interpretation of Coefficients in Nonlinear Regression Models**

The expected effect on $Y$ of a change in $X_1$ in a non-linear regression model.<br> Consider the nonlinear model
$$
Y_i = f(X_{1i}, X_{2i}, \dots, X_{ki}) + u_i \ , \ i=1,\dots,n,
$$
where, $f(X_{1i}, X_{2i}, \dots, X_{ki})$ is the population regression function and $u_i$ is the error term. <br>
Denote by ΔY the expected change in $Y$ associated with $\Delta{X_1}$, the change in $X_1$ while holding $X_2,\dots,X_k$ constant. That is, the expected change in $Y$ is the difference $$
\Delta{Y}=f(X_1+\Delta{X_1},X_2,⋯,X_k)−f(X_1,X_2, \dots, X_k).
$$
The estimator of this unknown population difference is the difference between the predicted values for these two cases. Let $\hat{f}(X_1,X_2, \ldots,X_k)$ be the predicted value of of $Y$ based on the estimator $\hat{f}$ of the population regression function. Then the predicted change in $Y$ is 
$$
\Delta \widehat{Y} = \hat{f}(X_1 + \Delta X_1, X_2, \cdots, X_k) - \hat{f}(X_1, X_2, \cdots, X_k).$$



- **Question:** What is the effect of one unit change in `avginc` in the 25 quantile and 95 quantile income levels on `testscr`?
    1. Estimate the `quad_model`.
    2. Calculate the 25 and 95 quantile values for `avginc`.
    3. Use the formula to calculate the change in predicted value of test-score.
    
$$
\begin{align*}
\Delta{\hat{testscr_{25}}}= (\hat{\beta_0} + 
\hat{\beta_1}(\text{25 quantile value + 1 unit}) + \\
\hat{\beta_2}(\text{25 quantile value + 1 unit})^2)\\
- (\hat{\beta_0} + \hat{\beta_1}(\text{25 quantile value}) +\\ 
\hat{\beta_2}(\text{25 quantile value})^2)
\end{align*}
$$

and

$$
\begin{align*}
\Delta{\hat{testscr_{95}}}= (\hat{\beta_0} + \hat{\beta_1}(\text{95 quantile value + 1 unit}) +\\
\hat{\beta_2}(\text{95 quantile value + 1 unit})^2)\\
- (\hat{\beta_0} + \hat{\beta_1}(\text{95 quantile value}) + \\
\hat{\beta_2}(\text{95 quantile value})^2)
\end{align*}
$$



```{r, eval=FALSE}
# the model we already calculated
quad_model
```

```{r, warning=FALSE, message=FALSE}
# Calculate the 25 and 95 quantile values
q25 <- as.integer(quantile(caschool$avginc, 0.25))
q95 <- as.integer(quantile(caschool$avginc, 0.95))

# calculate prediced testscr
hat_testscr_25 <- predict(quad_model, newdata = data.frame(avginc = c(q25, q25+1)))
hat_testscr_95 <- predict(quad_model, newdata = data.frame(avginc = c(q95, q95+1)))

# calculate the difference
diff_hat_testscr_25 <- hat_testscr_25[2] -hat_testscr_25[1]
diff_hat_testscr_95 <- hat_testscr_95[2] -hat_testscr_95[1]

library(tidyverse)
data.frame(Predicted_testscr = c("25 quantile", "95 quantile"),
           Difference = c(diff_hat_testscr_25, diff_hat_testscr_95)) %>% kableExtra::kable() %>% kableExtra::kable_styling(bootstrap_options = c("striped"), position = "center", full_width = FALSE)
```


So, for the quadratic model, the expected change in `testscr` induced by an increase in income from 10 to 11 is about 2.96 points but an increase in income from 30 to 31 increases the predicted score by only 1.27. Hence, the slope of the estimated quadratic regression function is steeper at low levels of income than at higher levels.

## {.smaller}

**Logarithms**
<div style='display: grid; 
            text-align: center;
            background-color: antiquewhite'
>

<table>
<thead>
<tr class="header">
<th align="center">Case</th>
<th align="center">   Model Specification   </th>
<th align="center"> R code </th>
<th align="center">   Interpretation of $\beta_1$   </th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">I</td>
<td align="left">$Y_i = \beta_0 + \beta_1\ln(X_i) + u_i$</td>
<td align="left">`lm(testscr ~ log(avginc), data = caschool)`</td>
<td align="left">A $1\%$ change in $X$ is associated with a change in $Y$ of $0.01 \times \beta_1$.</td>
</tr>
<tr class="even">
<td align="left">II</td>
<td align="left">$\ln(Y_i) = \beta_0 + \beta_1 X_i + u_i$</td>
<td align="left">`lm(log(testscr) ~ avginc, data = caschool)`</td>
<td align="left">A change in $X$ by one unit ($\Delta X = 1$) is associated with a $100 \times \beta_1 \%$ change in $Y$.</td>
</tr>
<tr class="odd">
<td align="left">III</td>
<td align="left">$\ln(Y_i) = \beta_0 + \beta_1 \ln(X_i) + u_i$</td>
<td align="left">`lm(log(testscr) ~ log(avginc), data = caschool)`</td>
<td align="left">A $1\%$ change in $X$ is associated with a $\beta_1\%$ change in $Y$, so $\beta_1$ is the elasticity of $Y$ with respect to $X$.</td>
</tr>
</tbody>
</table>

# 5.2 Interactions between independent variables {#five-two}

**Interaction between two binary variables**

Consider the following model
$$
\hat{testscr} = \beta_0 + \beta_1HiSTR + \beta_2HiEL + \beta_3(HiSTR \times HiEL) + u_i\\
\begin{align*}
  HiSTR =& \, 
    \begin{cases}
      1, & \text{if str $ \geq 20$} \\
      0, & \text{else},
    \end{cases} \\
  \\
  HiEL =& \,
    \begin{cases}
      1, & \text{if el_pct $ \geq 10$} \\
      0, & \text{else}.
    \end{cases}
\end{align*}
$$    

Here, $\beta_3$ measures the difference in the effect of having high student ratio and english learners as opposed to those districts having low `str` and more native english speakers.

## {.smaller}

...contd
```{r, message=FALSE, warning=FALSE}
# create and append the binary variables
caschool$HiSTR <- ifelse(caschool$str >= 20, 1, 0)
caschool$HiEL <- ifelse(caschool$el_pct >=10, 1, 0)

# the model
BinaryInteraction <- lm(testscr ~ HiSTR + HiEL + (HiSTR*HiEL), data = caschool)

# robust summary
RobustSummaryBinaryInteraction <-lmtest::coeftest(BinaryInteraction, 
                vcov. = sandwich::vcovHC(BinaryInteraction, type = "HC1"))

stargazer::stargazer(RobustSummaryBinaryInteraction, type = "text", out = "RobustBinaryInteraction.txt")
```



## {.smaller}


- The predicted effect of moving from a district with a low student–teacher ratio to one with a high student–teacher ratio, holding constant whether the percentage of English learners is high or low is $-1.9 - 3.5HiEL$. That is, if the fraction of English learners is low $(HiEL = 0)$, then the effect on test scores of moving from $HiSTR = 0$ to $HiSTR = 1$ is for test scores to decline by 1.9 points. If the fraction of English learners is high, then test scores are estimated to decline by $1.9 + 3.5 = 5.4$ points.


- Accordingly, the sample average test score for districts with low student–teacher ratios $(HiSTR_i = 0)$ and low fractions of English learners $(HiEL_i = 0)$ is 664.1. For districts with $HiSTR_i = 1$ (high student–teacher ratios) and $HiEL_i = 0$ (low fractions of English learners), the sample average is 662.2 (= 664.1 - 1.9). When $HiSTR_i = 0$ and $HiEL_i = 1$, the sample average is 645.9 (= 664.1 - 18.2), and when $HiSTR_i = 1$ and $HiEL_i = 1$, the sample average is 640.5 (= 664.1 - 1.9 - 18.2 - 3.5).



**Interactions between a binary and a continuous variable**

- **Question:** Whether the effect on test scores `testscr` of decreasing the student-teacher ratio `str` depends on whether there are many or few English learners `HiEL`. 
- Consider the following model:
$$
\hat{testscr_i} = \beta_0 + \beta_1str_i+ \beta_2HiEL_i + \beta_3(HiEL_i\times str_i) + u_i
$$

## {.smaller}


```{r}
# estimate the model
BinContInteract <- lm(testscr ~ str + HiEL + (HiEL*str), data = caschool)

# robust summary of the model
RobustSummaryBinContInteract <- lmtest::coeftest(
  BinContInteract, vcov. = sandwich::vcovHC(BinContInteract, type = "HC1"))

stargazer::stargazer(RobustSummaryBinContInteract, type = "text", out = "RobustSummaryBinContInteract.txt")

```


## {.smaller}

- The regression model is 
$$
\hat{testscr} = 682.24 - 0.97str+5.64HiEL-1.28(str \times HiEL)
$$
- For $HiEL=1$, the regression is
$$
\hat{testscr} = 682.24 - 0.97str+5.64-1.28str
$$
- For $HiEl=0$, the regression is
$$
\hat{testscr} = 682.24 - 0.97str
$$

---


```{r HiE, fig.cap="HiEL=0 vs HiEL=1"}
plot(caschool[caschool$HiEL == 0,c(14, 11)], col = "red", 
     ylim = c(min(caschool$testscr), max(caschool$testscr)) )
points(caschool[caschool$HiEL == 1,c(14, 11)], col = "blue")
abline(coef = c(RobustSummaryBinContInteract[1], 
                RobustSummaryBinContInteract[2]), col = "red")
abline(coef = c((RobustSummaryBinContInteract[1] + RobustSummaryBinContInteract[3]), 
                (RobustSummaryBinContInteract[2] + RobustSummaryBinContInteract[4])), 
       col = "blue")
legend("topright",legend = c(expression(HiEL==0), expression(HiEL==1)),
       lty = 1, col = c("red", "blue"))
```



**Interactions between two continuous variables**

- Interaction between `str` and `el_pct`.
```{r}
# the model
ContinuousInteraction <- lm(testscr ~ str + el_pct + (str*el_pct), data = caschool)

# robust estimates summary
RobustSummaryContinuousInteraction <- lmtest::coeftest(
  ContinuousInteraction, vcov. = sandwich::vcovHC(ContinuousInteraction, type = "HC1")
)
stargazer::stargazer(RobustSummaryContinuousInteraction, type = "text")
```



# 5.3 Example: The demand for economics journals {#five-three}


- **Question:**
    - How elastic is the demand by libraries for economics journals? 
- **What shall we do?**
    - Analyse the relationship between the *number of subscriptions* `(subs)` to a journal at U.S. libraries $(Y_i)$ and the journal’s library subscription price using data for the year 2000 for 180 economics journals. 
- **Data source**:
    - `data(Journals, package = "AER")`
    - `?Journals()` for description of variables.

## {.smaller}

**Relationship between *number of subscriptions* `(subs)` and *price per citation* `(PricePerCitation)`**

```{r}
# Create and append `PricePerCitation` 
data("Journals", package = "AER")
journals <- Journals
journals$PricePerCitation <- Journals$price / Journals$citations
# plot
plot(subs ~ PricePerCitation, data = journals)
```

## {.smaller}


- Because we are interested in estimating elasticities, we use a log-log specification.
- The scatterplot for log-log specification is given below:
```{r}
plot(log(subs)~log(PricePerCitation), data = journals)
abline(lm(log(subs) ~ log(PricePerCitation), data = journals))

```



We shall now estimate the following models and summarise the output in a table.
$$
\begin{align*}
  \text{A}\quad \ln(subs_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + u_i \\
  \text{B}\quad \ln(subs_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_4 \ln(Age_i)\\
  +& \beta_6 \ln(Characters_i) + u_i \\
\text{C}\quad \ln(subs_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_2 \ln(PricePerCitation_i)^2 \\
  +& \, \beta_3 \ln(PricePerCitation_i)^3 + \beta_4 \ln(Age_i) \\+& \beta_5 \left[\ln(Age_i) \times \ln(PricePerCitation_i)\right] \\ +& \, \beta_6 \ln(Characters_i) + u_i \\
\text{D}\quad \ln(subs_i) =& \, \beta_0 + \beta_1 \ln(PricePerCitation_i) + \beta_4 \ln(Age_i)\\ +& \beta_6 \ln(Characters_i) + u_i
\end{align*}
$$
 
where; <br>
$subs$- is the number of library subscriptions.<br>
$PricePerCitation$ - is the price per citation. <br>
$Age$ - is the age of the journal (we have to create and append this variable) <br>
$Characters$ - is the number of characters per year in the journal.




```{r}
# create and append the variables
# we already have `PricePerCitation` and `subs`.
journals$Age <- 2000 - journals$foundingyear
journals$Characters <- journals$charpp * journals$pages/1000000

# the data that we need
head(journals[,c(9, 11:13)])
```

## {.smaller}


```{r}
# estimate the models
A <- lm(log(subs) ~ log(PricePerCitation), data = journals)
B <- lm(log(subs) ~ log(PricePerCitation) + log(Age) + log(Characters), data = journals)
C <- lm(log(subs) ~ log(PricePerCitation) + I(log(PricePerCitation)^2) +
          I(log(PricePerCitation)^3) + log(Age) + log(Age)*log(PricePerCitation) +
          log(Characters) , data = journals)
D <-lm(log(subs) ~ log(PricePerCitation) + log(Age) + log(Age)*log(PricePerCitation) + log(Characters), data = journals)

# robust standard error estimates
library(sandwich)
robustSE <- list(
  sqrt(diag(vcovHC(A, type = "HC1"))),
  sqrt(diag(vcovHC(B, type = "HC1"))),
  sqrt(diag(vcovHC(C, type = "HC1"))),
  sqrt(diag(vcovHC(D, type = "HC1")))
)
# stargazer
stargazer::stargazer(A, B, C, D, type = "text",
                     se = robustSE,
                     digits = 3, 
                     column.labels = c("A", "B", "C", "D"))
```






## {.smaller}

***F*-test for significance of cubic terms**
```{r}
car::linearHypothesis(model = C, 
  hypothesis.matrix = c("I(log(PricePerCitation)^2)=0",
                        "I(log(PricePerCitation)^3)=0"),
    vcov. = sandwich::vcovHC(C, type = "HC1"))
```

We cannot reject the null hypothesis $H_0:β_3=β_4=0$ in model $C$.


## {.smaller}


What can we conclude from the regression?

1. The elasticity of demand for economics journals depend on the age of the journal. 
2. The evidence supports a linear, rather than a cubic, function of log price.
3. Demand is greater for journals with more characters, holding price and age constant.

## {.smaller}
Do you think older journals have a higher price elasticity?

```{r}
# get the coefficients from model D
d <- D$coefficients; d[1];d[2]; d[3]; d[4]; d[5]
```

 
## {.smaller}

```{r}
## plot
plot(log(subs) ~ log(PricePerCitation),data = journals)
abline(c((d[1] + d[3]*log(80)), (d[2] + d[5]*log(80))), col = "red")
abline(c((d[1] + d[3]*log(5)), (d[2] + d[5]*log(5))), col = "blue")
legend("bottomleft",
       legend = c("Demand when Age = 80", "Demand when Age = 5"),
       lty = 1, col = c("red", "blue"))


```

# 5.4 Nonlinear Effects on Test Scores of the Student-Teacher Ratio {#five-four}
This section addresses three specific questions about test scores and the student–teacher ratio. 

1. First, after controlling for differences in economic characteristics of different districts, does the effect on test scores of reducing the student–teacher ratio depend on the fraction of English learners? 
2. Second, does this effect depend on the value of the student–teacher ratio? 
3. Third, and most important, after taking economic factors and nonlinearities into account, what is the estimated effect on test scores of reducing the student–teacher ratio by two students per teacher, as our superintendent from Chapter 4 proposes to do?

## {.smaller}
**1. First, after controlling for differences in economic characteristics of different districts, does the effect on test scores of reducing the student–teacher ratio depend on the fraction of English learners? **
```{r}
LinearModel <- lm(testscr ~ str + HiEL + HiEL:str + meal_pct + log(avginc), 
    data = caschool)
```
Here, `meal_pct` and `avginc` measure the economic characteristics. <br>
Also, `HiEL` interact with `str` to measure whether higher proportion of english learners and better class size can improve test scores or not.

**2. Second, does this effect depend on the value of the student–teacher ratio? **
```{r}
CubicModel <- lm(testscr ~ str + I(str^2) + I(str^3) + HiEL + meal_pct + log(avginc), 
    data = caschool)
```
Here, the cubic `str` is included to lay emphasis on the value of `str` on test scores. 

## {.smaller}
```{r}
stargazer::stargazer(LinearModel, CubicModel, type = "text", out = "NLE.txt")
```



## {.smaller}
Does the cubic model fit better?
```{r, fig.cap="Figure 1"}
plot(testscr ~ str, data = caschool, pch = 19, col = rgb(0,0,1, alpha = 0.5))
lines(smooth.spline(caschool$str, predict(LinearModel)), lwd = 2, lty = 1, col = "green")
lines(smooth.spline(caschool$str, predict(CubicModel)), lwd = 2, lty = 2, col = "red")
legend("topright", legend = c("LinearModel", "CubicModel"), 
       lty = c(1,2), col = c("green", "red"), cex = 0.8)
```


## {.smaller}

```{r}
# F-test
car::linearHypothesis(model = CubicModel, 
  hypothesis.matrix = c( "I(str^2)=0","I(str^3)=0"),
    vcov. = sandwich::vcovHC(CubicModel, type = "HC1"))

```


## {.smaller}

**Summary of findings**

1. First, after controlling for economic background, whether there are many or few English learners in the district does not have a substantial influence on the effect on test scores of a change in the student–teacher ratio. In the linear specifications `LinearModel`, there is no statistically significant evidence of such a difference. The cubic specification in `CubicModel` provides statistically significant evidence (at the 5% level) that the regression functions are different for districts with high and low percentages of English learners; as shown in Figure 1, however, the estimated regression functions have similar slopes in the range of student–teacher ratios containing most of our data.

2. Second, after controlling for economic background, there is evidence of a nonlinear effect on test scores of the student–teacher ratio. This effect is statistically significant at the 1% level (the coefficients on $STR^2$ and $STR^3$ are always significant at the 1% level).


## {.smaller}



<ol start=3>
<li>The effect on test scores of reducing the student–teacher ratio by two students per teacher.
</ol>
If her district currently has a student–teacher ratio of 22 and she is considering cutting it to 20, then based on `CubicModel` the estimated effect of this reduction is to improve test scores by 1.93 points.
```{r}
l <- CubicModel$coefficients
improved_testscr1 <- (l[2]*20 + l[3]*(20^2) + l[4]*(20^3)) - 
   (l[2]*22 + l[3]*(22^2) + l[4]*(22^3))
names(improved_testscr1) <- "ImprovesTestScoreBy"; improved_testscr1
```

Based on the `LinearModel`, the result is 1.063`.
```{r}
improved_testscr2 <- LinearModel$coefficients[2]*20 - 
   LinearModel$coefficients[2]*22
names(improved_testscr2) <- "ImprovesTestScoreBy"; improved_testscr2
```

# 5.5 Exercises {#five-five}

Solve all the Empirical Exercises that you shall find in pages 352-355 in the book by Stock and Watson.
